"""
Process the Darshan daily summary json generated by the summarize_darshanlogs
CLI tool and generate a scoreboard of top sources of I/O based on user, file
system, and application.
"""

import os
import re
import sys
import json
import gzip
import mimetypes
import collections
import argparse

import tokio.config

def process_darshan_perfs(summary_jsons,
                          limit_fs=[], limit_user=[], limit_exe=[],
                          exclude_fs=[], exclude_user=[], exclude_exe=[],
                          tuples=False):
    """
    Ingest the per-log file system summary contained in the summary_json file(s)
    and produce a dictionary with bytes read/written reduced on application
    binary name, user name, and file system.
    """

    summary = {}
    for summary_json in summary_jsons:
        _, encoding = mimetypes.guess_type(summary_json)
        if encoding == 'gzip':
            summary.update(json.load(gzip.open(summary_json, 'r')))
        else:
            summary.update(json.load(open(summary_json, 'r')))

    regex_filename = re.compile(r'^([^_]+)_(.*?)_id(\d+)_.*.darshan')

    results = collections.OrderedDict()
    results['per_user'] = collections.defaultdict(lambda: collections.defaultdict(int))
    results['per_fs'] = collections.defaultdict(lambda: collections.defaultdict(int))
    results['per_exe'] = collections.defaultdict(lambda: collections.defaultdict(int))
    results['per_user_exe_fs'] = collections.defaultdict(lambda: collections.defaultdict(int))

    for darshan_log, counters in summary.items():
        darshan_log_bn = os.path.basename(darshan_log)
        regex_match = regex_filename.search(darshan_log_bn)
        if regex_match:
            username = regex_match.group(1)
            exename = regex_match.group(2)
        elif '_' in darshan_log_bn:
            username = darshan_log_bn.split('_', 1)[0]
            exename = "<unknown>"
        else:
            username = "<unknown>"
            exename = "<unknown>"

        # apply limits, if applicable
        if (limit_user and username not in limit_user) \
        or (limit_exe and exename not in limit_exe) \
        or (exclude_user and username in exclude_user) \
        or (exclude_exe and exename in exclude_exe):
            continue

        # precompile regular expressions
        mount_to_fsname = {}
        for rex_str, fsname in tokio.config.CONFIG.get('mount_to_fsname', {}).items():
            mount_to_fsname[re.compile(rex_str)] = fsname

        for mount in counters:
            if mount == '/':
                continue

            # try to map mount to a logical file system name
            fs_key = None
            for mount_rex, fsname in mount_to_fsname.items():
                match = mount_rex.match(mount)
                if match:
                    fs_key = fsname

            # if limit_fs/exclude_fs in play, filter at the per-record basis
            if limit_fs:
                if mount not in limit_fs \
                and (fs_key and fs_key not in limit_fs):
                    continue
            if exclude_fs:
                if mount in exclude_fs \
                or (fs_key and fs_key in exclude_fs):
                    continue

            if fs_key is None:
                fs_key = mount

            results['per_user'][username]['read_bytes'] += counters[mount].get('read_bytes', 0)
            results['per_user'][username]['write_bytes'] += counters[mount].get('write_bytes', 0)
            results['per_user'][username]['num_jobs'] += 1
            results['per_fs'][fs_key]['read_bytes'] += counters[mount].get('read_bytes', 0)
            results['per_fs'][fs_key]['write_bytes'] += counters[mount].get('write_bytes', 0)
            results['per_fs'][fs_key]['num_jobs'] += 1
            results['per_exe'][exename]['read_bytes'] += counters[mount].get('read_bytes', 0)
            results['per_exe'][exename]['write_bytes'] += counters[mount].get('write_bytes', 0)
            results['per_exe'][exename]['num_jobs'] += 1

            if tuples:
                key = "%.12s/%.12s/%.12s" % (username, exename, fs_key)
                results['per_user_exe_fs'][key]['read_bytes'] += counters[mount].get('read_bytes', 0)
                results['per_user_exe_fs'][key]['write_bytes'] += counters[mount].get('write_bytes', 0)
                results['per_user_exe_fs'][key]['num_jobs'] += 1

    return results

def print_top(categorized_data, max_show=10):
    """
    Print the biggest I/O {users, exes, file systems}
    """
    names = {
        'per_fs': "File Systems",
        'per_user': "Users",
        'per_exe': "Applications",
        'per_user_exe_fs': "User/App/FS",
    }

    categories = 0
    for category, rankings in categorized_data.items():
        print_buffer = ""
        name = names.get(category, category)
        if categories > 0:
            print_buffer += "\n"
        print_buffer += "%2s  %40s %10s %10s %8s\n" % ('#', name, 'Read(GiB)', 'Write(GiB)', '# Jobs')
        print_buffer += '=' * 75 + "\n"
        displayed = 0
        for winner in sorted(rankings, key=lambda x, r=rankings: r[x]['read_bytes'] + r[x]['write_bytes'], reverse=True):
            if len(winner) > 40:
                winner_str = "..." + winner[-37:]
            else:
                winner_str = winner
            displayed += 1
            if displayed > max_show:
                break
            print_buffer += "%2d. %40.40s %10.1f %10.1f %8d\n" % (displayed,
                                                      winner_str,
                                                      rankings[winner]['read_bytes'] / 2.0**30,
                                                      rankings[winner]['write_bytes'] / 2.0**30,
                                                      rankings[winner]['num_jobs'])
        if displayed > 0:
            sys.stdout.write(print_buffer)

        categories += 1

def main(argv=None):
    """Entry point for the CLI interface
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("summaryjson", type=str, nargs='+',
                        help="json output of darshan_per_fs_bytes.py")
    parser.add_argument("--json", action='store_true',
                        help="output in json format")
    parser.add_argument("--tuples", action='store_true',
                        help="generate top user-fs-exe tuples")
    parser.add_argument("--max-show", type=int, default=10,
                        help="show top N users, apps, file systems")
    group_fs = parser.add_mutually_exclusive_group()
    group_fs.add_argument("--limit-fs", type=str, default=None,
                          help="only process data targeting this file system")
    group_fs.add_argument("--exclude-fs", type=str, default=None,
                          help="exclude data targeting this file system")
    group_user = parser.add_mutually_exclusive_group()
    group_user.add_argument("--limit-user", type=str, default=None,
                            help="only process logs generated by this user")
    group_user.add_argument("--exclude-user", type=str, default=None,
                            help="exclude logs generated by this user")
    group_exe = parser.add_mutually_exclusive_group()
    group_exe.add_argument("--limit-exe", type=str, default=None,
                           help="only process logs generated by this binary")
    group_exe.add_argument("--exclude-exe", type=str, default=None,
                           help="exclude logs generated by this binary")

    args = parser.parse_args(argv)

    kwargs = {
        'limit_user': args.limit_user.split(',') if args.limit_user else [],
        'limit_fs': args.limit_fs.split(',') if args.limit_fs else [],
        'limit_exe': args.limit_exe.split(',') if args.limit_exe else [],
        'exclude_user': args.exclude_user.split(',') if args.exclude_user else [],
        'exclude_fs': args.exclude_fs.split(',') if args.exclude_fs else [],
        'exclude_exe': args.exclude_exe.split(',') if args.exclude_exe else [],
        'tuples': args.tuples,
    }

    results = process_darshan_perfs(args.summaryjson, **kwargs)
    if args.json:
        print(json.dumps(results, indent=4, sort_keys=True))
    else:
        print_top(results, max_show=args.max_show)
