"""
Process the Darshan daily summary generated by either summarize_darshanlogs
or index_darshanlogs tools and generate a scoreboard of top sources of I/O based
on user, file system, and/or application.
"""

import os
import re
import sys
import json
import gzip
import sqlite3
import mimetypes
import collections
import argparse

import tokio.config

TOP_USERS_QUERY = """
SELECT
    h.username,
    SUM(s.bytes_read) AS readbytes,
    SUM(s.bytes_written) AS writebytes,
    COUNT(h.filename)
FROM
    summaries AS s
INNER JOIN
    headers AS h ON h.log_id = s.log_id
GROUP BY h.username
ORDER BY (readbytes+writebytes) DESC
"""

TOP_FS_QUERY = """
SELECT
    m.mountpt,
    SUM(s.bytes_read) AS readbytes,
    SUM(s.bytes_written) AS writebytes,
    COUNT(h.filename)
FROM
    summaries AS s
INNER JOIN
    headers AS h ON h.log_id = s.log_id,
    mounts AS m ON m.fs_id = s.fs_id
GROUP BY m.mountpt
ORDER BY (readbytes+writebytes) DESC
"""

TOP_EXE_QUERY = """
SELECT
    h.exename,
    SUM(s.bytes_read) AS readbytes,
    SUM(s.bytes_written) AS writebytes,
    COUNT(s.bytes_written)
FROM
    summaries AS s
INNER JOIN
    headers AS h ON h.log_id = s.log_id
GROUP BY h.exename
ORDER BY (readbytes+writebytes) DESC
"""

TOP_TUPLE_QUERY = """
SELECT
    h.username, h.exename, m.mountpt,
    SUM(s.bytes_read) AS readbytes,
    SUM(s.bytes_written) AS writebytes,
    COUNT(s.bytes_written)
FROM
    summaries AS s
INNER JOIN
    headers AS h ON h.log_id = s.log_id,
    mounts AS m ON m.fs_id = s.fs_id
GROUP BY h.username, h.exename, m.mountpt
ORDER BY (readbytes+writebytes) DESC
"""

def query_index_db(db_filenames,
                   limit_fs=None, limit_user=None, limit_exe=None,
                   exclude_fs=None, exclude_user=None, exclude_exe=None,
                   tuples=False,
                   max_results=None):
    """Reduce Darshan log index by fs, user, and/or exe
    """
    fs_limits = []

    fsname_to_mount = {}
    for mount, fsname in tokio.config.CONFIG.get('mount_to_fsname', {}).items():
        fsname_to_mount[fsname] = mount

    if limit_fs:
        for limit in limit_fs:
            fs_limits.append("m.mountpt = '%s'" % fsname_to_mount.get(limit, limit))
    if exclude_fs:
        for limit in exclude_fs:
            fs_limits.append("m.mountpt != '%s'" % fsname_to_mount.get(limit, limit))

    user_limits = []
    if limit_user:
        for limit in limit_user:
            user_limits.append("h.username = '%s'" % limit)
    if exclude_user:
        for limit in exclude_user:
            user_limits.append("h.username != '%s'" % limit)

    exe_limits = []
    if limit_exe:
        for limit in limit_exe:
            exe_limits.append("h.exename = '%s'" % limit)
    if exclude_exe:
        for limit in exclude_exe:
            exe_limits.append("h.exename != '%s'" % limit)

    results = {}

    for db_filename in db_filenames:
        conn = sqlite3.connect(db_filename)
        cursor = conn.cursor()

        # 'per_user': TOP_USERS_QUERY,
        query = TOP_USERS_QUERY
        if exe_limits or user_limits:
            query = query.replace("GROUP",
                                  "WHERE\n    " + "\n    AND ".join(exe_limits + user_limits) + "\nGROUP")
        if max_results:
            query += "\nLIMIT %d" % max_results
        print(query)
        cursor.execute(query)
        if 'per_user' not in results:
            results['per_user'] = []
        results['per_user'] += cursor.fetchall()

        # 'per_fs': TOP_FS_QUERY,
        query = TOP_FS_QUERY
        if fs_limits:
            query = query.replace("GROUP",
                                  "WHERE\n    " + "\n    AND ".join(fs_limits) + "\nGROUP")
        if max_results:
            query += "\nLIMIT %d" % max_results
        print(query)
        cursor.execute(query)
        if 'per_fs' not in results:
            results['per_fs'] = []
        results['per_fs'] += cursor.fetchall()

        # 'per_exe': TOP_EXE_QUERY,
        query = TOP_EXE_QUERY
        if exe_limits or user_limits:
            query = query.replace("GROUP",
                                  "WHERE\n    " + "\n    AND ".join(exe_limits + user_limits) + "\nGROUP")
        if max_results:
            query += "\nLIMIT %d" % max_results
        print(query)
        cursor.execute(query)
        if 'per_exe' not in results:
            results['per_exe'] = []
        results['per_exe'] += cursor.fetchall()

        if tuples:
            # 'per_user_exe_fs': TOP_TUPLE_QUERY,
            query = TOP_TUPLE_QUERY
            if exe_limits or user_limits or fs_limits:
                query = query.replace("GROUP",
                                      "WHERE\n    " + "\n    AND ".join(exe_limits + user_limits + fs_limits) + "\nGROUP")
            if max_results:
                query += "\nLIMIT %d" % max_results
            print(query)
            cursor.execute(query)
            if 'per_user_exe_fs' not in results:
                results['per_user_exe_fs'] = []
            results['per_user_exe_fs'] += cursor.fetchall()

        cursor.close()
        conn.close()

    return results

def reduce_summary_jsons(summary_jsons,
                         limit_fs=None, limit_user=None, limit_exe=None,
                         exclude_fs=None, exclude_user=None, exclude_exe=None,
                         tuples=False):
    """Reduces summary JSON by fs, user, and/or exe

    Ingests the per-log file system summary contained in the summary_json
    file(s) and produce a dictionary with bytes read/written reduced on
    application binary name, user name, and file system.
    """

    summary = {}
    for summary_json in summary_jsons:
        _, encoding = mimetypes.guess_type(summary_json)
        if encoding == 'gzip':
            summary.update(json.load(gzip.open(summary_json, 'r')))
        else:
            summary.update(json.load(open(summary_json, 'r')))

    regex_filename = re.compile(r'^([^_]+)_(.*?)_id(\d+)_.*.darshan')

    results = collections.OrderedDict()
    results['per_user'] = collections.defaultdict(lambda: collections.defaultdict(int))
    results['per_fs'] = collections.defaultdict(lambda: collections.defaultdict(int))
    results['per_exe'] = collections.defaultdict(lambda: collections.defaultdict(int))
    results['per_user_exe_fs'] = collections.defaultdict(lambda: collections.defaultdict(int))

    for darshan_log, counters in summary.items():
        darshan_log_bn = os.path.basename(darshan_log)
        regex_match = regex_filename.search(darshan_log_bn)
        if regex_match:
            username = regex_match.group(1)
            exename = regex_match.group(2)
        elif '_' in darshan_log_bn:
            username = darshan_log_bn.split('_', 1)[0]
            exename = "<unknown>"
        else:
            username = "<unknown>"
            exename = "<unknown>"

        # apply limits, if applicable
        if (limit_user and username not in limit_user) \
        or (limit_exe and exename not in limit_exe) \
        or (exclude_user and username in exclude_user) \
        or (exclude_exe and exename in exclude_exe):
            continue

        # precompile regular expressions
        mount_to_fsname = {}
        for rex_str, fsname in tokio.config.CONFIG.get('mount_to_fsname', {}).items():
            mount_to_fsname[re.compile(rex_str)] = fsname

        for mount in counters:
#           if mount == '/':
#               continue

            # try to map mount to a logical file system name
            fs_key = None
            for mount_rex, fsname in mount_to_fsname.items():
                match = mount_rex.match(mount)
                if match:
                    fs_key = fsname

            # if limit_fs/exclude_fs in play, filter at the per-record basis
            if limit_fs:
                if mount not in limit_fs \
                and (fs_key and fs_key not in limit_fs):
                    continue
            if exclude_fs:
                if mount in exclude_fs \
                or (fs_key and fs_key in exclude_fs):
                    continue

            if fs_key is None:
                fs_key = mount

            results['per_user'][username]['read_bytes'] += counters[mount].get('read_bytes', 0)
            results['per_user'][username]['write_bytes'] += counters[mount].get('write_bytes', 0)
            results['per_user'][username]['num_jobs'] += 1
            results['per_fs'][fs_key]['read_bytes'] += counters[mount].get('read_bytes', 0)
            results['per_fs'][fs_key]['write_bytes'] += counters[mount].get('write_bytes', 0)
            results['per_fs'][fs_key]['num_jobs'] += 1
            results['per_exe'][exename]['read_bytes'] += counters[mount].get('read_bytes', 0)
            results['per_exe'][exename]['write_bytes'] += counters[mount].get('write_bytes', 0)
            results['per_exe'][exename]['num_jobs'] += 1

            if tuples:
                key = "%.12s/%.12s/%.12s" % (username, exename, fs_key)
                results['per_user_exe_fs'][key]['read_bytes'] += counters[mount].get('read_bytes', 0)
                results['per_user_exe_fs'][key]['write_bytes'] += counters[mount].get('write_bytes', 0)
                results['per_user_exe_fs'][key]['num_jobs'] += 1

    # convert dict of dicts into a list of tuples
    results_lists = {}
    for category, rankings in results.items():
        results_lists[category] = [(key, val.get('read_bytes', 0), val.get('write_bytes', 0), val.get('num_jobs', 0)) for key, val in rankings.items()]

    return results_lists

def print_top(categorized_data, max_show=10):
    """
    Print the biggest I/O {users, exes, file systems}
    """
    names = {
        'per_fs': "File Systems",
        'per_user': "Users",
        'per_exe': "Applications",
        'per_user_exe_fs': "User/App/FS",
    }

    categories = 0
    for category, rankings in categorized_data.items():
        print_buffer = ""
        name = names.get(category, category)
        if categories > 0:
            print_buffer += "\n"
        print_buffer += "%2s  %40s %10s %10s %8s\n" % ('#', name, 'Read(GiB)', 'Write(GiB)', '# Jobs')
        print_buffer += '=' * 75 + "\n"
        displayed = 0
        for winner in sorted(rankings, key=lambda x: x[1] + x[2], reverse=True):
            if len(winner[0]) > 40:
                winner_str = "..." + winner[0][-37:]
            else:
                winner_str = winner[0]
            displayed += 1
            if displayed > max_show:
                break
            print_buffer += "%2d. %40.40s %10.1f %10.1f %8d\n" % (displayed,
                                                                  winner_str,
                                                                  winner[1] / 2.0**30,
                                                                  winner[2] / 2.0**30,
                                                                  winner[3])
        if displayed > 0:
            sys.stdout.write(print_buffer)

        categories += 1

def main(argv=None):
    """Entry point for the CLI interface
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("indexfile", type=str, nargs='+',
                        help="json output of darshan_per_fs_bytes.py")
    parser.add_argument("--parser-json", action='store_true',
                        help="indexfile is JSON instead of SQLite")
    parser.add_argument("--json", action='store_true',
                        help="output in json format")
    parser.add_argument("--tuples", action='store_true',
                        help="generate top user-fs-exe tuples")
    parser.add_argument("--max-show", type=int, default=10,
                        help="show top N users, apps, file systems")
    group_fs = parser.add_mutually_exclusive_group()
    group_fs.add_argument("--limit-fs", type=str, default=None,
                          help="only process data targeting this file system")
    group_fs.add_argument("--exclude-fs", type=str, default=None,
                          help="exclude data targeting this file system")
    group_user = parser.add_mutually_exclusive_group()
    group_user.add_argument("--limit-user", type=str, default=None,
                            help="only process logs generated by this user")
    group_user.add_argument("--exclude-user", type=str, default=None,
                            help="exclude logs generated by this user")
    group_exe = parser.add_mutually_exclusive_group()
    group_exe.add_argument("--limit-exe", type=str, default=None,
                           help="only process logs generated by this binary")
    group_exe.add_argument("--exclude-exe", type=str, default=None,
                           help="exclude logs generated by this binary")

    args = parser.parse_args(argv)

    kwargs = {
        'limit_user': args.limit_user.split(',') if args.limit_user else [],
        'limit_fs': args.limit_fs.split(',') if args.limit_fs else [],
        'limit_exe': args.limit_exe.split(',') if args.limit_exe else [],
        'exclude_user': args.exclude_user.split(',') if args.exclude_user else [],
        'exclude_fs': args.exclude_fs.split(',') if args.exclude_fs else [],
        'exclude_exe': args.exclude_exe.split(',') if args.exclude_exe else [],
        'tuples': args.tuples,
    }

    if args.parser_json:
        results = reduce_summary_jsons(args.indexfile, **kwargs)
    else:
        results = query_index_db(args.indexfile, **kwargs)

    if args.json:
        print(json.dumps(results, indent=4, sort_keys=True))
    else:
        print_top(results, max_show=args.max_show)
