#!/usr/bin/env python
"""Retrieve data generated by collectd and stored in Elasticsearch

This module is a combination of a generic Elasticsearch connection handler
and a set of queries specific to retrieving data generated by collectd.
"""

import copy
import time
import json
import tokio
try:
    from elasticsearch import Elasticsearch
except ImportError:
    pass

QUERY_DISK_DATA = {
    "query": {
        "constant_score": {
            "filter": {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "@timestamp": {}
                            }
                        },
                        {
                            "prefix": {
                                "hostname": "bb"
                            }
                        },
                        {
                            "prefix": {
                                "plugin_instance": "nvme"
                            }
                        },
                        {
                            "regexp": {
                                "collectd_type": "disk_(octets|ops)"
                            }
                        },
                        {
                            "term": {
                                "plugin": "disk"
                            }
                        }
                    ]
                }
            }
        }
    }
}
QUERY_CPU_DATA = {
    "query": {
        "constant_score": {
            "filter": {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "@timestamp": {}
                            }
                        },
                        {
                            "prefix": {
                                "hostname": "bb"
                            }
                        },
                        {
                            "term": {
                                "plugin": "cpu"
                            }
                        },
                        {
                            "regexp": {
                                "type_instance": "(idle|user|system)"
                            }
                        },
                    ]
                }
            }
        }
    }
}
QUERY_MEMORY_DATA = {
    "query": {
        "constant_score": {
            "filter": {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "@timestamp": {}
                            }
                        },
                        {
                            "prefix": {
                                "hostname": "bb"
                            }
                        },
                        {
                            "term": {
                                "plugin": "memory"
                            }
                        },

                    ]
                }
            }
        }
    }
}


### Only return the following _source fields
COLLECTD_SOURCE_FILTER = [
    '@timestamp',
    'hostname',
    'plugin',
    'collectd_type',
    'type_instance',
    'plugin_instance',
    'value',
    'longterm',
    'midterm',
    'shortterm',
    'majflt',
    'minflt',
    'if_octets',
    'if_packets',
    'if_errors',
    'rx',
    'tx',
    'read',
    'write',
    'io_time',
]


class CollectdEs(object):
    """collectd-Elasticsearch connection handler.

    Wrapper around an Elasticsearch connection context that provides simpler
    scrolling functionality for very long documents and callback functions to be
    run after each page is retrieved.
    """
    def __init__(self, host, port, index=None, scroll_size='1m', page_size=10000, timeout=30):
        """Configure and connect to an Elasticsearch endpoint.

        Args:
            host (str): hostname for the Elasticsearch REST endpoint
            port (int): port where Elasticsearch REST endpoint is bound
            index (str): name of index against which queries will be issued
            scroll_size (str): how long to keep the scroll search context open
                (e.g., "1m" for 1 minute)
            page_size (int): how many documents should be returned per scrolled
                page (e.g., 10000 for 10k docs per scroll)
            timeout (int): how many seconds to wait for a response from
                Elasticsearch before the query should time out

        Attributes:
            client: Elasticsearch connection handler
            page (dict): last page retrieved by a query
            scroll_pages (list): dictionary of pages retrieved by query
            index (str): name of index against which queries will be issued
            connect_host (str): hostname for Elasticsearch REST endpoint
            connect_port (int): port where Elasticsearch REST endpoint is bound
            connect_timeout (int): seconds before query should time out
            page_size (int): max number of documents returned per page
            scroll_size (int): duration to keep scroll search context open
            scroll_id: identifier for the scroll search context currently in use
            num_flushes (int): number of times the flush function has been
                called
            sort_by (str): field by which Elasticsearch should sort results
                before returning them as query results
        """
        # retain state of Elasticsearch client
        self.client = None
        self.page = None
        self.scroll_pages = []
        self.index = index
        # connection info
        self.connect_host = host
        self.connect_port = port
        self.connect_timeout = timeout
        # for the scroll API
        self.page_size = page_size
        self.scroll_size = scroll_size
        self.scroll_id = None
        # for query_and_scroll
        self.num_flushes = 0
        # hidden parameters to refine how Elasticsearch queries are issued
        self.sort_by = ''

        self.connect()

    def connect(self):
        """Instantiate a connection and retain the connection context.
        """
        self.client = Elasticsearch(host=self.connect_host,
                                    port=self.connect_port,
                                    timeout=self.connect_timeout)

    def close(self):
        """Close and invalidate the connection context.
        """
        if self.client:
            self.client = None

    def query(self, query):
        """Issue an Elasticsearch query.

        Issues a query and returns the resulting page.  If the query included a
        scrolling request, the `scroll_id` attribute is set so that scrolling
        can continue.

        Args:
            query (dict): Dictionary representing the query to issue

        Returns:
            dict: The page resulting from the issued query.
        """
        self.page = self.client.search(body=query, index=self.index, sort=self.sort_by)
        if '_scroll_id' in self.page:
            self.scroll_id = self.page['_scroll_id']
        return self.page

    def scroll(self):
        """Request the next page of results.

        Requests the next page of results for a query that is scrolling.
        This can only be performed if the `scroll_id` attribute is set (e.g.,
        by the ``query()`` method).

        Returns:
            dict: The next page in the scrolling context.
        """
        if self.scroll_id is None:
            raise Exception('no scroll id')
        self.page = self.client.scroll(scroll_id=self.scroll_id, scroll=self.scroll_size)
        return self.page

    def query_and_scroll(self, query, source_filter=True, filter_function=None,
                         flush_every=None, flush_function=None):
        """Issue a query and retain all results.

        Issues a query and scrolls through every resulting page, optionally
        applying in situ logic for filtering and flushing.  All resulting pages
        are appended to the ``scroll_pages`` attribute of this object.

        The ``scroll_pages`` attribute must be wiped by whatever is consuming it;
        if this does not happen, `query_and_scroll()` will continue appending
        results to the results of previous queries.

        Args:
            source_filter (bool or list): Return all fields contained in each
                document's _source field if True; otherwise, only return source
                fields contained in the provided list of str.
            filter_function (function, optional): Function to call before each
                set of results is appended to the ``scroll_pages`` attribute; if
                specified, return value of this function is what is appended.
            flush_every (int or None): trigger the flush function once the
                number of docs contained across all ``scroll_pages`` reaches
                this value.  If None, do not apply `flush_function`.
            flush_function (function, optional): function to call when
                `flush_every` docs are retrieved.
        """

        def process_page(scroll_state):
            """Pull down a page, lightly filter it, and attach it to this object's page list.
            """
            # nonlocal is Python 3 only...
            # nonlocal total_hits
            # nonlocal num_hits_since_flush

            if not self.page['hits']['hits']:
                return False

            self.scroll_id = self.page['_scroll_id']
            num_hits = len(self.page['hits']['hits'])
            scroll_state['total_hits'] += num_hits

            # if this page will push us over flush_every, flush it first
            if flush_function is not None \
            and flush_every \
            and (scroll_state['num_hits_since_flush'] + num_hits) > flush_every:
                flush_function(self)
                scroll_state['num_hits_since_flush'] = 0
                self.num_flushes += 1

            # increment hits since flush only after we've (possibly) flushed
            scroll_state['num_hits_since_flush'] += num_hits

            # if a filter function exists, use its output as the page to append
            if filter_function is None:
                filtered_page = self.page
            else:
                filtered_page = filter_function(self.page)

            # finally append the page
            self.scroll_pages.append(filtered_page)
            return True

        # initialize the scroll state
        self.scroll_pages = []
        # note that we use a dict here because we need to manipulate these
        # values from within a nested function, and Python's scoping rules are
        # rather arbitrary and there's no way to pass individual variables by
        # reference
        scroll_state = {
            'total_hits': 0,
            'num_hits_since_flush': 0
        }

        # Get first set of results and a scroll id
        self.page = self.client.search(
            index=self.index,
            body=query,
            scroll=self.scroll_size,
            size=self.page_size,
            _source=source_filter,
        )

        more_results = process_page(scroll_state)

        while more_results:
            self.page = self.scroll()
            more_results = process_page(scroll_state)

    def query_disk(self, start, end):
        """Query Elasticsearch for collectd disk plugin data.

        Args:
            start (datetime.datetime): lower bound for query (inclusive)
            end (datetime.datetime): upper bound for query (exclusive)
        """
        self.query_timeseries(QUERY_DISK_DATA, start, end)

    def query_memory(self, start, end):
        """Query Elasticsearch for collectd memory plugin data.

        Args:
            start (datetime.datetime): lower bound for query (inclusive)
            end (datetime.datetime): upper bound for query (exclusive)
        """
        self.query_timeseries(QUERY_MEMORY_DATA, start, end)

    def query_cpu(self, start, end):
        """Query Elasticsearch for collectd cpu plugin data.

        Args:
            start (datetime.datetime): lower bound for query (inclusive)
            end (datetime.datetime): upper bound for query (exclusive)
        """
        self.query_timeseries(QUERY_CPU_DATA, start, end)

    def query_timeseries(self, query_template, start, end):
        """Query Elasticsearch for collectd plugin data.

        Args:
            start (datetime.datetime): lower bound for query (inclusive)
            end (datetime.datetime): upper bound for query (exclusive)
        """
        query = build_timeseries_query(query_template, start, end)

        ### Print query
        tokio.debug.debug_print(json.dumps(query, indent=4))

        ### Run query
        time0 = time.time()
        self.query_and_scroll(
            query=query,
            source_filter=COLLECTD_SOURCE_FILTER,
            filter_function=lambda x: x['hits']['hits'],
            flush_every=50000,
            flush_function=lambda x: x,
        )
        tokio.debug.debug_print("Elasticsearch query took %s seconds" % (time.time() - time0))

def build_timeseries_query(orig_query, start, end):
    """Create a query object with time ranges bounded.

    Given a query dict and a start/end datetime object, return a new query
    object with the correct time ranges bounded.  Relies on `orig_query`
    containing at least one ``@timestamp`` field to indicate where the time
    ranges should be inserted.

    Args:
        orig_query (dict): A query object containing at least one ``@timestamp``
            field.
        start (datetime.datetime): lower bound for query (inclusive)
        end (datetime.datetime): upper bound for query (exclusive)

    Returns:
        dict: A query object with all instances of ``@timestamp`` bounded by
        `start` and `end`.
    """
    def map_item(obj, target_key, map_function):
        """
        Recursively walk a hierarchy of dicts and lists, searching for a
        matching key.  For each match found, apply map_function to that key's
        value.
        """
        if isinstance(obj, list):
            iterator = enumerate
            if target_key in obj:
                return obj[target_key]
        elif isinstance(obj, dict):
            iterator = dict.iteritems
            if target_key in obj:
                return obj[target_key]
        else:
            # hit a dead end without a match
            return None
        # if this isn't a dead end, search down each iterable element
        for _, value in iterator(obj):
            if isinstance(value, (list, dict)):
                # dive down any discovered rabbit holes
                item = map_item(value, target_key, map_function)
                if item is not None:
                    map_function(item)
        return None

    def set_time_range(time_range_obj, time_format="epoch_second"):
        """
        Set the upper and lower bounds of a time range
        """
        time_range_obj['gte'] = int(time.mktime(start.timetuple()))
        time_range_obj['lt'] = int(time.mktime(end.timetuple()))
        time_range_obj['format'] = time_format

    query = copy.deepcopy(orig_query)

    map_item(query, '@timestamp', set_time_range)

    return query
