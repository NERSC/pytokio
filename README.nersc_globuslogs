INSERT INTO transfers (timestamp, block, buffer, code, date, dest, destip, file, host, nbytes, start, streams, stripes, taskid, type, user, volume, bandwidth_mbps, duration, start_date, end_date, loghost) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?) ('2019-02-01T18:00:57.635Z', 4194304, 332800, ['226', '226'], '20190201180057.343719', ['128.55.205.29', '128.55.205.29'], ['128.55.205.29', '128.55.205.29'], '/global/projecta/projectdirs/lsst/production/DC2_ImSim/Run2.0i/instance_catalog/00190288/Dynamic/specFileSN_MS_10066_3929_59835.2529_y.dat', 'dtn10.nersc.gov', 35295, '20190201180057.310850', 4, '1', ['b947a070-26RIPES=1', 'b947a070-2637-11e9-934f-0e3d676669f4'], ['STOR', 'RETR'], 'heatherk', '/', 8.56, 0.033, '2019-02-01T18:00:57.310Z', '2019-02-01T18:00:57.343Z', None)
Traceback (most recent call last):
  File "./bin/archive_nersc_globuslogs.py", line 6, in <module>
    main()
  File "/global/homes/g/glock/src/git/pytokio-dev/tokio/cli/archive_nersc_globuslogs.py", line 158, in main
    query_to_sqlite(esdb, query_start, query_end, conn, args.table)
  File "/global/homes/g/glock/src/git/pytokio-dev/tokio/cli/archive_nersc_globuslogs.py", line 90, in query_to_sqlite
    cursor.execute(query, insert)
sqlite3.InterfaceError: Error binding parameter 3 - probably unsupported type.

The actual bad record from Elasticsearch is

DATE=20190201180057.343719 HOST=dtn10.nersc.gov PROG=globus-gridftp-server NL.EVNT=FTP_INFO START=20190201180057.310850 USER=heatherk FILE=/global/projecta/projectdirs/lsst/production/DC2_ImSim/Run2.0i/instance_catalog/00190288/Dynamic/specFileSN_MS_10066_3929_59835.2529_y.dat BUFFER=332800 BLOCK=4194304 NBYTES=35295 VOLUME=/ STREAMS=4 STRIPES=1 DEST=[128.55.205.29] TYPE=STOR CODE=226 TASKID=b947a070-26RIPES=1 DEST=[128.55.205.29] TYPE=RETR CODE=226 TASKID=b947a070-2637-11e9-934f-0e3d676669f4 retrans=0,0,0,0

And as a result, Kibana throws back a few of these fields as JSON lists instead
of scalars.  Should we just discard malformed records and pretend like they
didn't happen?  It's otherwise impossible to discern the true contents of the
record since bits are missing.  We may also have to validate all fields (incl.
TASKID) before inserting to ensure we don't pollute the SQLite database.

Bummer.


UPDATE March 20, 2019:

I added type checking to _archive_nersc_globuslogs.py_ and it does skip a bunch
of them, but it still manages to fail on this:

    Traceback (most recent call last):
      File "./bin/archive_nersc_globuslogs.py", line 6, in <module>
        main()
      File "/global/homes/g/glock/src/git/pytokio-dev/tokio/cli/archive_nersc_globuslogs.py", line 166, in main
        query_to_sqlite(esdb, query_start, query_end, conn, args.table)
      File "/global/homes/g/glock/src/git/pytokio-dev/tokio/cli/archive_nersc_globuslogs.py", line 100, in query_to_sqlite
        cursor.executemany(query, inserts)
    sqlite3.InterfaceError: Error binding parameter 4 - probably unsupported type.

This error can be generated by querying between

    ./bin/archive_nersc_globuslogs.py 2019-03-15T02:00:00 2019-03-15T03:00:00 --output tmp.db

(I think--it definitely happens between March 15 and March 16)
